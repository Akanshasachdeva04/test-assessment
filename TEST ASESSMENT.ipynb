{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a775a8-4566-4e00-a478-4647f3159101",
   "metadata": {},
   "source": [
    "# Step 1: Import  libraries \n",
    "\n",
    "Import libraries such as Pandas, NumPy, and Scikit-learn, which are commonly used for data preprocessing.\n",
    "                                                                                 \n",
    "# Step 2: Handle missing values\n",
    "\n",
    "Check for missing values in the dataset using the isnull() or isna() function.\n",
    "Decide on a strategy to handle missing values, such as:\n",
    "Dropping rows or columns with missing values (if the dataset is large and missing values are sparse).\n",
    "Filling missing values with a specific value (e.g., mean, median, or mode) using the fillna() function.\n",
    "Imputing missing values using a machine learning algorithm (e.g., K-Nearest Neighbors or decision trees).\n",
    "Implement the chosen strategy to handle missing values.\n",
    "\n",
    "    \n",
    "# Step 3: Data cleaning and normalization\n",
    "\n",
    "Check for outliers and anomalies in the data using visualization techniques (e.g., histograms, scatter plots) or statistical methods (e.g., Z-score, modified Z-score).\n",
    "Remove or transform outliers and anomalies as necessary.\n",
    "Normalize or scale the data to prevent features with large ranges from dominating the model. Common techniques include:\n",
    "Standardization (Z-scoring): subtract the mean and divide by the standard deviation for each feature.\n",
    "Min-max scaling: rescale features to a common range (e.g., 0 to 1) using the\n",
    "\n",
    "                                                                                  \n",
    "                                                                                   \n",
    "# Step 4: Feature selection and engineering\n",
    "\n",
    "Identify relevant features and remove irrelevant or redundant ones using techniques such as:\n",
    "Correlation analysis: calculate the correlation between features and the target variable.\n",
    "Mutual information: calculate the mutual information between features and the target variable.\n",
    "Recursive feature elimination: recursively eliminate the least important features until a specified number of features is reached.\n",
    "\n",
    "# Step 5: Split data into training and testing sets\n",
    "\n",
    "Split the preprocessed data into training and testing sets using techniques such as:\n",
    "Random sampling: randomly split the data into training and testing sets.\n",
    "Stratified sampling: split the data into training and testing sets while maintaining the same class distribution.\n",
    "\n",
    "                                                                                   \n",
    "# Step 6: Verify data quality\n",
    "\n",
    "Verify that the preprocessed data meets the requirements for modeling, such as:\n",
    "No missing values.   \n",
    "Features are scaled and normalized.\n",
    "Outliers and anomalies are handled.\n",
    "Relevant features are selected and engineered.                                                                                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1468b1dc-b0de-4435-a8c5-4f7b0db744cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b2fbb3-9608-4112-bf9d-f1d1d79b8d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Missing values in each column:\n",
      " Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "file_path =(r\"creditcard(1).xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "X = df.drop(columns=['Class'])  \n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1543e1b0-902e-4961-82bc-85b41c12b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': {'Accuracy': 0.9991222218320986, 'Precision': 0.8636363636363636, 'Recall': 0.5816326530612245, 'F1 Score': 0.6951219512195121}, 'Random Forest': {'Accuracy': 0.9995611109160493, 'Precision': 0.974025974025974, 'Recall': 0.7653061224489796, 'F1 Score': 0.8571428571428571}, 'Decision Tree': {'Accuracy': 0.9991397773954567, 'Precision': 0.7247706422018348, 'Recall': 0.8061224489795918, 'F1 Score': 0.7632850241545894}, 'SVM': {'Accuracy': 0.9993153330290369, 'Precision': 0.9682539682539683, 'Recall': 0.6224489795918368, 'F1 Score': 0.7577639751552796}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    model_performance[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "print(model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d08db34a-0919-4096-b4eb-459790ec298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[56855     9]\n",
      " [   41    57]]\n",
      "Precision: 0.8636363636363636\n",
      "Recall: 0.5816326530612245\n",
      "F1 Score: 0.6951219512195121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "lr_model = models['Logistic Regression']\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_lr)\n",
    "recall = recall_score(y_test, y_pred_lr)\n",
    "f1 = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef548f52-ced2-4b15-a227-e2f0a9a4ba32",
   "metadata": {},
   "source": [
    "# QUE 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4790b3-fb20-45d4-b4e0-d4dbf036879a",
   "metadata": {},
   "source": [
    "To ensure a machine learning model for credit card fraud detection remains effective over time, use the following strategies:\n",
    "\n",
    " TECHNICAL STRATEGIES\n",
    "\n",
    "1. Regular Model Retraining: Update the model with new data periodically.\n",
    "2. Adaptive Algorithms: Use algorithms that adapt to changes in data distribution.\n",
    "3. Model Monitoring: Track performance metrics and set up alerts for significant drops.\n",
    "4. Data Pipeline Management: Maintain a robust data pipeline with quality checks.\n",
    "5. Ensemble Methods: Combine predictions from multiple models.\n",
    "6. Feature Engineering: Update features to capture emerging fraud patterns.\n",
    "\n",
    "\n",
    "  OPERATIONAL STRATEGIES\n",
    "\n",
    "1. Human-in-the-Loop: Use feedback from fraud analysts to refine the model.\n",
    "2. Continuous Evaluation: Regularly test the model on validation sets or with A/B testing.\n",
    "3. Collaboration with Fraud Analysts: Work closely with fraud detection teams.\n",
    "4. Model Governance: Document and manage changes to the model.\n",
    "5. Scalability and Performance: Ensure the infrastructure can handle increasing data volumes.\n",
    "6. Stakeholder Communication: Keep stakeholders informed about the modelâ€™s performance and updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f7bae-61ac-4bcc-b795-7b18a36fc9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
